{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation - GPS Spoofing Detection\n",
    "\n",
    "This notebook demonstrates model training and evaluation for GPS spoofing detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from utils.synthetic_data import generate_synthetic_dataset\n",
    "from preprocessing.signal_processing import generate_ca_code\n",
    "from features.pipeline import build_feature_vector, preprocess_features\n",
    "from models.train import train_model, evaluate_model, print_evaluation_report\n",
    "from models.persistence import save_model, load_model\n",
    "from utils.plots import plot_confusion_matrix, plot_roc_curves\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Dataset and Extract Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic signals (larger dataset for training)\n",
    "signals, labels, metadata = generate_synthetic_dataset(\n",
    "    num_authentic=200,\n",
    "    num_spoofed=200,\n",
    "    fs=5e6,\n",
    "    duration=0.5,\n",
    "    prn_range=(1, 5),\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Generated {len(signals)} signals\")\n",
    "print(f\"Class distribution: {np.bincount(labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features\n",
    "print(\"Extracting features...\")\n",
    "\n",
    "all_features = []\n",
    "for i, signal in enumerate(signals):\n",
    "    if i % 50 == 0:\n",
    "        print(f\"  Processing signal {i+1}/{len(signals)}...\")\n",
    "    \n",
    "    prn = metadata[i]['prn']\n",
    "    ca_code = generate_ca_code(prn)\n",
    "    \n",
    "    features = build_feature_vector(\n",
    "        signal=signal,\n",
    "        prn_code=ca_code,\n",
    "        fs=5e6,\n",
    "        label=labels[i],\n",
    "        metadata={'prn': prn, 'segment_index': i}\n",
    "    )\n",
    "    all_features.append(features)\n",
    "\n",
    "df_features = pd.DataFrame(all_features)\n",
    "print(f\"\\nFeature extraction complete. Shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare Data for Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and labels\n",
    "X = df_features.drop(columns=['label', 'prn', 'segment_index'], errors='ignore')\n",
    "y = df_features['label'].values\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTraining set class distribution: {np.bincount(y_train)}\")\n",
    "print(f\"Test set class distribution: {np.bincount(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess features\n",
    "X_train_processed, imputer, scaler, _ = preprocess_features(X_train, y_train, fit=True)\n",
    "X_test_processed, _, _, _ = preprocess_features(X_test, y_test, imputer=imputer, scaler=scaler, fit=False)\n",
    "\n",
    "print(f\"Preprocessed training shape: {X_train_processed.shape}\")\n",
    "print(f\"Preprocessed test shape: {X_test_processed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train Random Forest Model (Baseline)\n",
    "\n",
    "Train Random Forest with balanced class weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "print(\"Training Random Forest...\")\n",
    "rf_model, rf_train_metrics = train_model(\n",
    "    X_train_processed, y_train,\n",
    "    model_name='random_forest',\n",
    "    params={\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 15,\n",
    "        'min_samples_split': 5,\n",
    "        'class_weight': 'balanced',\n",
    "    },\n",
    "    use_smote=False,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Metrics:\")\n",
    "print(f\"CV Mean Accuracy: {rf_train_metrics['cv_mean']:.4f} (+/- {rf_train_metrics['cv_std']:.4f})\")\n",
    "print(f\"Training Accuracy: {rf_train_metrics['train_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Evaluate Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "rf_metrics = evaluate_model(rf_model, X_test_processed, y_test)\n",
    "\n",
    "# Print report\n",
    "print_evaluation_report(rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix\n",
    "plot_confusion_matrix(\n",
    "    np.array(rf_metrics['confusion_matrix']),\n",
    "    class_names=['Authentic', 'Spoofed'],\n",
    "    title='Random Forest - Confusion Matrix'\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train with SMOTE (Optional)\n",
    "\n",
    "Try training with SMOTE oversampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest with SMOTE\n",
    "print(\"Training Random Forest with SMOTE...\")\n",
    "rf_smote_model, rf_smote_train_metrics = train_model(\n",
    "    X_train_processed, y_train,\n",
    "    model_name='random_forest',\n",
    "    params={\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 15,\n",
    "        'class_weight': 'balanced',\n",
    "    },\n",
    "    use_smote=True,\n",
    "    cv=5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"\\nTraining Metrics (with SMOTE):\")\n",
    "print(f\"CV Mean Accuracy: {rf_smote_train_metrics['cv_mean']:.4f} (+/- {rf_smote_train_metrics['cv_std']:.4f})\")\n",
    "print(f\"Training Accuracy: {rf_smote_train_metrics['train_accuracy']:.4f}\")\n",
    "\n",
    "# Evaluate\n",
    "rf_smote_metrics = evaluate_model(rf_smote_model, X_test_processed, y_test)\n",
    "print_evaluation_report(rf_smote_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compare Multiple Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train SVM\n",
    "print(\"Training SVM...\")\n",
    "svm_model, svm_train_metrics = train_model(\n",
    "    X_train_processed, y_train,\n",
    "    model_name='svm',\n",
    "    params={'C': 1.0, 'gamma': 'scale'},\n",
    "    cv=3,  # Fewer folds for SVM (slower)\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "svm_metrics = evaluate_model(svm_model, X_test_processed, y_test)\n",
    "print(f\"SVM Test Accuracy: {svm_metrics['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare models\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['Random Forest', 'RF + SMOTE', 'SVM'],\n",
    "    'Accuracy': [\n",
    "        rf_metrics['accuracy'],\n",
    "        rf_smote_metrics['accuracy'],\n",
    "        svm_metrics['accuracy']\n",
    "    ],\n",
    "    'Precision': [\n",
    "        rf_metrics['precision'],\n",
    "        rf_smote_metrics['precision'],\n",
    "        svm_metrics['precision']\n",
    "    ],\n",
    "    'Recall': [\n",
    "        rf_metrics['recall'],\n",
    "        rf_smote_metrics['recall'],\n",
    "        svm_metrics['recall']\n",
    "    ],\n",
    "    'F1 Score': [\n",
    "        rf_metrics['f1_score'],\n",
    "        rf_smote_metrics['f1_score'],\n",
    "        svm_metrics['f1_score']\n",
    "    ],\n",
    "    'ROC AUC': [\n",
    "        rf_metrics['roc_auc'],\n",
    "        rf_smote_metrics['roc_auc'],\n",
    "        svm_metrics['roc_auc']\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(\"=\"*80)\n",
    "print(comparison.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC curves\n",
    "y_scores = {\n",
    "    'Random Forest': rf_model.predict_proba(X_test_processed)[:, 1],\n",
    "    'RF + SMOTE': rf_smote_model.predict_proba(X_test_processed)[:, 1],\n",
    "    'SVM': svm_model.predict_proba(X_test_processed)[:, 1]\n",
    "}\n",
    "\n",
    "plot_roc_curves(y_test, y_scores, title='Model Comparison - ROC Curves')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select best model based on F1 score\n",
    "best_idx = comparison['F1 Score'].idxmax()\n",
    "best_model_name = comparison.loc[best_idx, 'Model']\n",
    "\n",
    "print(f\"Best model: {best_model_name}\")\n",
    "print(f\"F1 Score: {comparison.loc[best_idx, 'F1 Score']:.4f}\")\n",
    "\n",
    "# Save the best model (Random Forest in this case)\n",
    "model_path = '../data/processed/best_model.pkl'\n",
    "metadata = {\n",
    "    'model_name': best_model_name,\n",
    "    'metrics': rf_metrics if best_idx == 0 else (rf_smote_metrics if best_idx == 1 else svm_metrics),\n",
    "    'features': list(X.columns),\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "best_model = rf_model if best_idx == 0 else (rf_smote_model if best_idx == 1 else svm_model)\n",
    "save_model(best_model, model_path, metadata)\n",
    "\n",
    "# Also save preprocessors\n",
    "import joblib\n",
    "joblib.dump(imputer, '../data/processed/imputer.pkl')\n",
    "joblib.dump(scaler, '../data/processed/scaler.pkl')\n",
    "print(\"Preprocessors saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Test Loading and Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "loaded_model, loaded_metadata = load_model(model_path)\n",
    "\n",
    "print(\"\\nLoaded model metadata:\")\n",
    "print(f\"Model: {loaded_metadata['model_name']}\")\n",
    "print(f\"Test Accuracy: {loaded_metadata['metrics']['accuracy']:.4f}\")\n",
    "\n",
    "# Test prediction\n",
    "sample_idx = 0\n",
    "X_sample = X_test_processed[sample_idx:sample_idx+1]\n",
    "y_sample = y_test[sample_idx]\n",
    "\n",
    "prediction = loaded_model.predict(X_sample)[0]\n",
    "probability = loaded_model.predict_proba(X_sample)[0]\n",
    "\n",
    "print(f\"\\nTest sample prediction:\")\n",
    "print(f\"True label: {'Spoofed' if y_sample == 1 else 'Authentic'}\")\n",
    "print(f\"Predicted: {'Spoofed' if prediction == 1 else 'Authentic'}\")\n",
    "print(f\"Probabilities: Authentic={probability[0]:.3f}, Spoofed={probability[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "**Best Model Performance:**\n",
    "- Model: Random Forest with balanced class weights\n",
    "- Achieves high accuracy on synthetic data\n",
    "- Key features: peak_to_secondary, cn0_estimate, fpw\n",
    "\n",
    "**Recommendations:**\n",
    "1. Test on real GPS spoofing datasets (FGI-SpoofRepo, TEXBAT)\n",
    "2. Fine-tune hyperparameters for specific scenarios\n",
    "3. Consider ensemble methods for improved robustness\n",
    "4. Monitor false alarm rate in deployment\n",
    "\n",
    "**Next Steps:**\n",
    "- Deploy model using `scripts/script_run_pipeline.py`\n",
    "- Integrate with real-time GPS receiver\n",
    "- Collect more diverse spoofing scenarios for training"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
